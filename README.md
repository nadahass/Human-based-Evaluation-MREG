# Human based Evaluation of Human and LLM-Generated Multimodal Referring Expressions (MRE)

To evaluate the success of multimodal referring expression generation (MREG) models, two human-based experiments will be conducted using a crowdsourcing platform, Amazon Mechanical Turk (AMT). 
We propose two primary criteria to assess how generative modules imbued with situational awareness and the ability to prompt non-verbal behavior could be compared with humans' generation capabilities. 
Criterion 1: how well the agent-generated strategies qualitatively compared to humans-generated strategies, as evaluated using a preference ordering method; 
Criterion 2: how well the agent-generated multimodal references quantitatively compared to humans-generated multimodal references, as evaluated using task completion.

## Study Design and Procedures

We selected 50 human MREs from the SCMRE dataset. These
were compared with 50 REs generated by the virtual agent in the same situation
when driven by a generative model trained over the human data. A total of 100
videos were collected. The referencing strategies examined for each of human and
IVA generation are pointing only REs, relational speech-only REs, attributive
speech-only REs, relational multimodal REs and attributive multimodal REs.
Videos were used in a set of AMT human intelligence tasks (HITs), each involving
workers rating 1 video for both fluency and clarity, including 1 machine generated
RE or 1 human REs, for a total of 100 HITs. Workers first identified the target
object mentioned, then, they rate the fluency of each video description on a
Likert scale (5 = best, 1 = worst). Each video was completed by 10 workers, for a
total of 1,000 individual judgments. We recruit workers fluent in English between
18 and 60 years old. They have given 1 hour per task and be compensated for
their time via the platform 0.75$ for each HIT.
